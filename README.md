# Detecting-Spam-Emails-Using-Tensorflow-in-Python


## ğŸš€ Project Overview  
Spam emails are unsolicited or unwanted messages sent in bulk, often cluttering inboxes and wasting user attention. This project focuses on building a **deep learningâ€“based email classification system** that can automatically distinguish between **Spam** and **Ham (Not Spam)** emails using **TensorFlow** and **Natural Language Processing (NLP)** techniques.

The entire implementation is intentionally kept inside a **single Jupyter Notebook**, making the project easy to follow, reproduce, and learn from.

---

## ğŸ¯ Objectives  
- **Automatically classify emails** as Spam or Ham  
- Apply **text preprocessing and NLP techniques**  
- Train an **LSTM-based deep learning model**  
- Achieve **high accuracy on unseen data**  

---

## ğŸ“‚ Dataset Information  
- **File Name:** `Emails.csv`  
- **Total Records:** 5,171 emails  
- **Classes:**  
  - `spam` â†’ Unwanted or promotional emails  
  - `ham` â†’ Legitimate emails  

âš ï¸ The dataset was **imbalanced**, so the majority class (Ham) was **downsampled** to ensure fair model training.

---

## ğŸ› ï¸ Tech Stack  
- **Language:** Python  
- **Libraries Used:**  
  - NumPy & Pandas (Data Handling)  
  - TensorFlow / Keras (Deep Learning)  
  - NLTK (Text Preprocessing)  
  - Scikit-learn (Train-Test Split)  
  - Matplotlib & Seaborn (Visualization)  
  - WordCloud (Text Insights)  

---

## ğŸ§  Approach & Workflow  

### ğŸ”¹ Data Preprocessing  
- Removed unnecessary email headers  
- Cleaned punctuation  
- Removed stopwords using NLTK  
- Normalized text for better learning  

### ğŸ”¹ Exploratory Data Analysis  
- Visualized class distribution  
- Generated **WordClouds** for Spam and Ham emails  

### ğŸ”¹ Text Vectorization  
- Tokenized text into numerical sequences  
- Applied padding to maintain equal sequence length  

---

## ğŸ—ï¸ Model Architecture  
The model is built using **Keras Sequential API** and includes:

- **Embedding Layer** â†’ Learns word representations  
- **LSTM Layer** â†’ Captures sequential patterns in text  
- **Dense Layers** â†’ Feature extraction  
- **Sigmoid Output Layer** â†’ Binary classification  

This architecture is well-suited for **text classification problems**.

---

## âš™ï¸ Model Training  
- **Optimizer:** Adam  
- **Loss Function:** Binary Cross-Entropy  
- **Callbacks Used:**  
  - EarlyStopping (prevents overfitting)  
  - ReduceLROnPlateau (adaptive learning rate)  

Training was performed for multiple epochs with validation monitoring.

---

## ğŸ“Š Results  
- âœ… **Test Accuracy:** **97%**  
- âœ… **Test Loss:** 0.1202  

The model generalizes well and performs strongly on unseen data.

---

## ğŸ“ Project Structure  

